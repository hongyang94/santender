{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import time, warnings, json, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalise_dataset(train, test):\n",
    "    print('Train shape:' + str(train.shape))\n",
    "    print('Test shape:' + str(test.shape))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded, time elapsed: 15.59230661392212\n",
      "Dataset loaded, time elapsed: 17.333653926849365\n",
      "Train shape:(76020, 371)\n",
      "Test shape:(75818, 370)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_file(filepath):\n",
    "\n",
    "    start_time = time.time()\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Dataset loaded, time elapsed: \" + str(elapsed_time))\n",
    "\n",
    "    return df\n",
    "\n",
    "train = load_file('../data/train.csv')  # (76020, 371)\n",
    "test = load_file('../data/test.csv')  # (75818, 370)\n",
    "verbalise_dataset(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicated features\n",
      "Train shape:(76020, 309)\n",
      "Test shape:(75818, 308)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_col(train, test):\n",
    "\n",
    "    print('Removing duplicated features')\n",
    "    output = []\n",
    "    columns = train.columns  # list of headers\n",
    "    for i in range(len(columns)-1):\n",
    "        for j in range(i+1,len(columns)):\n",
    "            if np.array_equal(train[columns[i]].values, train[columns[j]].values) and columns[j] not in output:\n",
    "                    output.append(columns[j])\n",
    "    \n",
    "    train = train.drop(output, axis=1)\n",
    "    test = test.drop(output, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "clean_train, clean_test = remove_duplicate_col(train, test)\n",
    "verbalise_dataset(clean_train, clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing constant features\n",
      "Train shape:(76020, 308)\n",
      "Test shape:(75818, 307)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_constant_col(train, test):\n",
    "\n",
    "    print('Removing constant features')\n",
    "    columns = []\n",
    "    for col in train.columns:\n",
    "        if train[col].std() == 0:\n",
    "            columns.append(col)\n",
    "\n",
    "    train = train.drop(columns, axis=1)\n",
    "    test = test.drop(columns, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "clean_train, clean_test = remove_constant_col(clean_train, clean_test)\n",
    "verbalise_dataset(clean_train, clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "X = clean_train.drop([\"TARGET\",\"ID\"],axis=1)\n",
    "Y = clean_train['TARGET'].values\n",
    "\n",
    "test_id = clean_test.ID\n",
    "test = clean_test.drop([\"ID\"],axis=1)\n",
    "seeds = [2534324, 13454236, 34623, 1367457, 12321]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8342  \u001b[0m | \u001b[0m 0.9345  \u001b[0m | \u001b[0m 0.01596 \u001b[0m | \u001b[0m 7.028   \u001b[0m | \u001b[0m 54.64   \u001b[0m | \u001b[0m 0.02701 \u001b[0m | \u001b[0m 21.35   \u001b[0m | \u001b[0m 0.03943 \u001b[0m | \u001b[0m 0.06914 \u001b[0m | \u001b[0m 0.8195  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8345  \u001b[0m | \u001b[95m 0.8545  \u001b[0m | \u001b[95m 0.01172 \u001b[0m | \u001b[95m 7.93    \u001b[0m | \u001b[95m 48.57   \u001b[0m | \u001b[95m 0.02514 \u001b[0m | \u001b[95m 34.93   \u001b[0m | \u001b[95m 0.0327  \u001b[0m | \u001b[95m 0.07505 \u001b[0m | \u001b[95m 0.8822  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8351  \u001b[0m | \u001b[95m 0.811   \u001b[0m | \u001b[95m 0.01541 \u001b[0m | \u001b[95m 8.35    \u001b[0m | \u001b[95m 57.07   \u001b[0m | \u001b[95m 0.02991 \u001b[0m | \u001b[95m 24.16   \u001b[0m | \u001b[95m 0.03105 \u001b[0m | \u001b[95m 0.07476 \u001b[0m | \u001b[95m 0.9812  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8362  \u001b[0m | \u001b[95m 0.8126  \u001b[0m | \u001b[95m 0.0175  \u001b[0m | \u001b[95m 7.329   \u001b[0m | \u001b[95m 46.07   \u001b[0m | \u001b[95m 0.01735 \u001b[0m | \u001b[95m 27.27   \u001b[0m | \u001b[95m 0.03911 \u001b[0m | \u001b[95m 0.07887 \u001b[0m | \u001b[95m 0.9795  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.9233  \u001b[0m | \u001b[0m 0.01777 \u001b[0m | \u001b[0m 8.507   \u001b[0m | \u001b[0m 58.11   \u001b[0m | \u001b[0m 0.01013 \u001b[0m | \u001b[0m 30.39   \u001b[0m | \u001b[0m 0.04956 \u001b[0m | \u001b[0m 0.0659  \u001b[0m | \u001b[0m 0.9153  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8345  \u001b[0m | \u001b[0m 0.8016  \u001b[0m | \u001b[0m 0.01413 \u001b[0m | \u001b[0m 7.044   \u001b[0m | \u001b[0m 38.01   \u001b[0m | \u001b[0m 0.01435 \u001b[0m | \u001b[0m 20.17   \u001b[0m | \u001b[0m 0.03951 \u001b[0m | \u001b[0m 0.06669 \u001b[0m | \u001b[0m 0.9119  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.9055  \u001b[0m | \u001b[0m 0.01606 \u001b[0m | \u001b[0m 8.199   \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.02759 \u001b[0m | \u001b[0m 34.98   \u001b[0m | \u001b[0m 0.03979 \u001b[0m | \u001b[0m 0.06391 \u001b[0m | \u001b[0m 0.9239  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8355  \u001b[0m | \u001b[0m 0.9076  \u001b[0m | \u001b[0m 0.01385 \u001b[0m | \u001b[0m 8.446   \u001b[0m | \u001b[0m 38.09   \u001b[0m | \u001b[0m 0.02165 \u001b[0m | \u001b[0m 34.89   \u001b[0m | \u001b[0m 0.04891 \u001b[0m | \u001b[0m 0.07259 \u001b[0m | \u001b[0m 0.8865  \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8368  \u001b[0m | \u001b[95m 0.8687  \u001b[0m | \u001b[95m 0.01836 \u001b[0m | \u001b[95m 8.777   \u001b[0m | \u001b[95m 38.06   \u001b[0m | \u001b[95m 0.01915 \u001b[0m | \u001b[95m 34.93   \u001b[0m | \u001b[95m 0.03233 \u001b[0m | \u001b[95m 0.07797 \u001b[0m | \u001b[95m 0.9392  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8353  \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 0.01356 \u001b[0m | \u001b[0m 8.343   \u001b[0m | \u001b[0m 38.02   \u001b[0m | \u001b[0m 0.01083 \u001b[0m | \u001b[0m 34.9    \u001b[0m | \u001b[0m 0.04592 \u001b[0m | \u001b[0m 0.06445 \u001b[0m | \u001b[0m 0.8498  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8958  \u001b[0m | \u001b[0m 0.0118  \u001b[0m | \u001b[0m 8.832   \u001b[0m | \u001b[0m 38.12   \u001b[0m | \u001b[0m 0.02533 \u001b[0m | \u001b[0m 34.92   \u001b[0m | \u001b[0m 0.04448 \u001b[0m | \u001b[0m 0.07062 \u001b[0m | \u001b[0m 0.9607  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8412  \u001b[0m | \u001b[0m 0.01431 \u001b[0m | \u001b[0m 8.643   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.01388 \u001b[0m | \u001b[0m 34.95   \u001b[0m | \u001b[0m 0.04146 \u001b[0m | \u001b[0m 0.06396 \u001b[0m | \u001b[0m 0.8257  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8322  \u001b[0m | \u001b[0m 0.917   \u001b[0m | \u001b[0m 0.01039 \u001b[0m | \u001b[0m 7.859   \u001b[0m | \u001b[0m 38.09   \u001b[0m | \u001b[0m 0.01762 \u001b[0m | \u001b[0m 20.12   \u001b[0m | \u001b[0m 0.03548 \u001b[0m | \u001b[0m 0.06632 \u001b[0m | \u001b[0m 0.8011  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8337  \u001b[0m | \u001b[0m 0.8587  \u001b[0m | \u001b[0m 0.01036 \u001b[0m | \u001b[0m 7.826   \u001b[0m | \u001b[0m 59.99   \u001b[0m | \u001b[0m 0.01082 \u001b[0m | \u001b[0m 34.98   \u001b[0m | \u001b[0m 0.03152 \u001b[0m | \u001b[0m 0.07133 \u001b[0m | \u001b[0m 0.9179  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8347  \u001b[0m | \u001b[0m 0.8691  \u001b[0m | \u001b[0m 0.01005 \u001b[0m | \u001b[0m 7.84    \u001b[0m | \u001b[0m 38.02   \u001b[0m | \u001b[0m 0.02042 \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.04447 \u001b[0m | \u001b[0m 0.07545 \u001b[0m | \u001b[0m 0.8807  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "{'colsample_bytree': 0.8686856726608465, 'learning_rate': 0.018357312633415893, 'max_depth': 8.776690926792526, 'min_child_weight': 38.057213200748144, 'min_split_gain': 0.019152377549519914, 'num_leaves': 34, 'reg_alpha': 0.032333445747500786, 'reg_lambda': 0.07797239150112883, 'subsample': 0.9391692064639641}\n"
     ]
    }
   ],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "    test_pred_proba = np.zeros(len(X))\n",
    "    \n",
    "    n_splits = 10\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2534324)\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "        # Feature selection\n",
    "        clf = lgb.LGBMClassifier()\n",
    "        selector = clf.fit(X, Y)\n",
    "        fs = SelectFromModel(selector, prefit=True)\n",
    "        train_df = fs.transform(X)\n",
    "        test_df = fs.transform(test)\n",
    "\n",
    "        train_x, train_y = train_df[train_idx], Y[train_idx]\n",
    "        valid_x, valid_y = train_df[valid_idx], Y[valid_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params, n_estimators=100, nthread=4)\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(Y, test_pred_proba)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # set to ignore warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': (.01, .02),\n",
    "        'num_leaves': (20, 35),\n",
    "        'colsample_bytree': (0.8, 1),\n",
    "        'subsample': (0.8, 1),\n",
    "        'max_depth': (7, 9),\n",
    "        'reg_alpha': (.03, .05),\n",
    "        'reg_lambda': (.06, .08),\n",
    "        'min_split_gain': (.01, .03),\n",
    "        'min_child_weight': (38, 60)\n",
    "    }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "    bo.maximize(init_points=5, n_iter=10)\n",
    "    best_params = bo.max['params']\n",
    "    \n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "\n",
    "    print(best_params)\n",
    "    # output copied and stored as params.json\n",
    "    \n",
    "    import json\n",
    "    json.dump(best_params, open('params0.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8357  \u001b[0m | \u001b[0m 0.9047  \u001b[0m | \u001b[0m 0.01831 \u001b[0m | \u001b[0m 7.939   \u001b[0m | \u001b[0m 50.29   \u001b[0m | \u001b[0m 0.01127 \u001b[0m | \u001b[0m 29.09   \u001b[0m | \u001b[0m 0.04976 \u001b[0m | \u001b[0m 0.07681 \u001b[0m | \u001b[0m 0.8951  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8355  \u001b[0m | \u001b[0m 0.804   \u001b[0m | \u001b[0m 0.01725 \u001b[0m | \u001b[0m 8.783   \u001b[0m | \u001b[0m 51.81   \u001b[0m | \u001b[0m 0.0104  \u001b[0m | \u001b[0m 26.1    \u001b[0m | \u001b[0m 0.04386 \u001b[0m | \u001b[0m 0.06591 \u001b[0m | \u001b[0m 0.9893  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.832   \u001b[0m | \u001b[0m 0.9728  \u001b[0m | \u001b[0m 0.01259 \u001b[0m | \u001b[0m 7.008   \u001b[0m | \u001b[0m 49.28   \u001b[0m | \u001b[0m 0.02042 \u001b[0m | \u001b[0m 22.09   \u001b[0m | \u001b[0m 0.03606 \u001b[0m | \u001b[0m 0.07453 \u001b[0m | \u001b[0m 0.8347  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8353  \u001b[0m | \u001b[0m 0.8257  \u001b[0m | \u001b[0m 0.01588 \u001b[0m | \u001b[0m 7.097   \u001b[0m | \u001b[0m 54.85   \u001b[0m | \u001b[0m 0.01727 \u001b[0m | \u001b[0m 27.24   \u001b[0m | \u001b[0m 0.0425  \u001b[0m | \u001b[0m 0.07836 \u001b[0m | \u001b[0m 0.9037  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8338  \u001b[0m | \u001b[0m 0.9153  \u001b[0m | \u001b[0m 0.01455 \u001b[0m | \u001b[0m 7.71    \u001b[0m | \u001b[0m 55.07   \u001b[0m | \u001b[0m 0.0142  \u001b[0m | \u001b[0m 24.44   \u001b[0m | \u001b[0m 0.03822 \u001b[0m | \u001b[0m 0.06851 \u001b[0m | \u001b[0m 0.9114  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8355  \u001b[0m | \u001b[0m 0.9579  \u001b[0m | \u001b[0m 0.0168  \u001b[0m | \u001b[0m 8.873   \u001b[0m | \u001b[0m 38.09   \u001b[0m | \u001b[0m 0.01153 \u001b[0m | \u001b[0m 34.9    \u001b[0m | \u001b[0m 0.04701 \u001b[0m | \u001b[0m 0.0684  \u001b[0m | \u001b[0m 0.8817  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8342  \u001b[0m | \u001b[0m 0.9513  \u001b[0m | \u001b[0m 0.01634 \u001b[0m | \u001b[0m 8.942   \u001b[0m | \u001b[0m 59.91   \u001b[0m | \u001b[0m 0.02384 \u001b[0m | \u001b[0m 34.93   \u001b[0m | \u001b[0m 0.04317 \u001b[0m | \u001b[0m 0.07898 \u001b[0m | \u001b[0m 0.8879  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.8368  \u001b[0m | \u001b[95m 0.8794  \u001b[0m | \u001b[95m 0.01996 \u001b[0m | \u001b[95m 7.582   \u001b[0m | \u001b[95m 38.01   \u001b[0m | \u001b[95m 0.02646 \u001b[0m | \u001b[95m 34.93   \u001b[0m | \u001b[95m 0.03517 \u001b[0m | \u001b[95m 0.06875 \u001b[0m | \u001b[95m 0.8487  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8359  \u001b[0m | \u001b[0m 0.8859  \u001b[0m | \u001b[0m 0.01688 \u001b[0m | \u001b[0m 8.815   \u001b[0m | \u001b[0m 38.01   \u001b[0m | \u001b[0m 0.01174 \u001b[0m | \u001b[0m 34.96   \u001b[0m | \u001b[0m 0.04569 \u001b[0m | \u001b[0m 0.06116 \u001b[0m | \u001b[0m 0.8851  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8352  \u001b[0m | \u001b[0m 0.9531  \u001b[0m | \u001b[0m 0.01564 \u001b[0m | \u001b[0m 7.201   \u001b[0m | \u001b[0m 38.03   \u001b[0m | \u001b[0m 0.0245  \u001b[0m | \u001b[0m 34.96   \u001b[0m | \u001b[0m 0.03586 \u001b[0m | \u001b[0m 0.0691  \u001b[0m | \u001b[0m 0.8094  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8364  \u001b[0m | \u001b[0m 0.8201  \u001b[0m | \u001b[0m 0.01671 \u001b[0m | \u001b[0m 8.851   \u001b[0m | \u001b[0m 38.05   \u001b[0m | \u001b[0m 0.02867 \u001b[0m | \u001b[0m 34.91   \u001b[0m | \u001b[0m 0.03025 \u001b[0m | \u001b[0m 0.07747 \u001b[0m | \u001b[0m 0.9927  \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.8369  \u001b[0m | \u001b[95m 0.8239  \u001b[0m | \u001b[95m 0.01944 \u001b[0m | \u001b[95m 8.676   \u001b[0m | \u001b[95m 38.02   \u001b[0m | \u001b[95m 0.02963 \u001b[0m | \u001b[95m 34.97   \u001b[0m | \u001b[95m 0.03661 \u001b[0m | \u001b[95m 0.07712 \u001b[0m | \u001b[95m 0.8397  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8565  \u001b[0m | \u001b[0m 0.01222 \u001b[0m | \u001b[0m 7.212   \u001b[0m | \u001b[0m 38.14   \u001b[0m | \u001b[0m 0.0256  \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.03625 \u001b[0m | \u001b[0m 0.07686 \u001b[0m | \u001b[0m 0.8443  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8353  \u001b[0m | \u001b[0m 0.9599  \u001b[0m | \u001b[0m 0.01963 \u001b[0m | \u001b[0m 8.976   \u001b[0m | \u001b[0m 38.02   \u001b[0m | \u001b[0m 0.02427 \u001b[0m | \u001b[0m 20.18   \u001b[0m | \u001b[0m 0.0484  \u001b[0m | \u001b[0m 0.06505 \u001b[0m | \u001b[0m 0.8637  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8358  \u001b[0m | \u001b[0m 0.8248  \u001b[0m | \u001b[0m 0.01456 \u001b[0m | \u001b[0m 8.61    \u001b[0m | \u001b[0m 38.05   \u001b[0m | \u001b[0m 0.02198 \u001b[0m | \u001b[0m 34.98   \u001b[0m | \u001b[0m 0.04247 \u001b[0m | \u001b[0m 0.07277 \u001b[0m | \u001b[0m 0.8877  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "{'colsample_bytree': 0.8238618368964905, 'learning_rate': 0.019439683480406353, 'max_depth': 8.67620873589349, 'min_child_weight': 38.02225586598372, 'min_split_gain': 0.029629396213956806, 'num_leaves': 34, 'reg_alpha': 0.03661338391757757, 'reg_lambda': 0.07711969344064172, 'subsample': 0.8397116165868461}\n"
     ]
    }
   ],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "    test_pred_proba = np.zeros(len(X))\n",
    "    \n",
    "    n_splits = 10\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=13454236)\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "        # Feature selection\n",
    "        clf = lgb.LGBMClassifier()\n",
    "        selector = clf.fit(X, Y)\n",
    "        fs = SelectFromModel(selector, prefit=True)\n",
    "        train_df = fs.transform(X)\n",
    "        test_df = fs.transform(test)\n",
    "\n",
    "        train_x, train_y = train_df[train_idx], Y[train_idx]\n",
    "        valid_x, valid_y = train_df[valid_idx], Y[valid_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params, n_estimators=100, nthread=4)\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(Y, test_pred_proba)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # set to ignore warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': (.01, .02),\n",
    "        'num_leaves': (20, 35),\n",
    "        'colsample_bytree': (0.8, 1),\n",
    "        'subsample': (0.8, 1),\n",
    "        'max_depth': (7, 9),\n",
    "        'reg_alpha': (.03, .05),\n",
    "        'reg_lambda': (.06, .08),\n",
    "        'min_split_gain': (.01, .03),\n",
    "        'min_child_weight': (38, 60)\n",
    "    }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "    bo.maximize(init_points=5, n_iter=10)\n",
    "    best_params = bo.max['params']\n",
    "    \n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "\n",
    "    print(best_params)\n",
    "    # output copied and stored as params.json\n",
    "    \n",
    "    import json\n",
    "    json.dump(best_params, open('params1.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8347  \u001b[0m | \u001b[0m 0.8414  \u001b[0m | \u001b[0m 0.01835 \u001b[0m | \u001b[0m 7.551   \u001b[0m | \u001b[0m 59.61   \u001b[0m | \u001b[0m 0.01476 \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 0.03484 \u001b[0m | \u001b[0m 0.06161 \u001b[0m | \u001b[0m 0.8317  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8346  \u001b[0m | \u001b[0m 0.8574  \u001b[0m | \u001b[0m 0.01535 \u001b[0m | \u001b[0m 7.52    \u001b[0m | \u001b[0m 39.4    \u001b[0m | \u001b[0m 0.02904 \u001b[0m | \u001b[0m 25.05   \u001b[0m | \u001b[0m 0.04591 \u001b[0m | \u001b[0m 0.06508 \u001b[0m | \u001b[0m 0.9961  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8344  \u001b[0m | \u001b[0m 0.9869  \u001b[0m | \u001b[0m 0.0183  \u001b[0m | \u001b[0m 8.44    \u001b[0m | \u001b[0m 39.95   \u001b[0m | \u001b[0m 0.01208 \u001b[0m | \u001b[0m 23.59   \u001b[0m | \u001b[0m 0.032   \u001b[0m | \u001b[0m 0.07836 \u001b[0m | \u001b[0m 0.9315  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8345  \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 0.01598 \u001b[0m | \u001b[0m 7.669   \u001b[0m | \u001b[0m 47.79   \u001b[0m | \u001b[0m 0.01836 \u001b[0m | \u001b[0m 27.55   \u001b[0m | \u001b[0m 0.04255 \u001b[0m | \u001b[0m 0.07719 \u001b[0m | \u001b[0m 0.9376  \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.8348  \u001b[0m | \u001b[95m 0.8074  \u001b[0m | \u001b[95m 0.01762 \u001b[0m | \u001b[95m 8.783   \u001b[0m | \u001b[95m 53.28   \u001b[0m | \u001b[95m 0.02314 \u001b[0m | \u001b[95m 22.15   \u001b[0m | \u001b[95m 0.04489 \u001b[0m | \u001b[95m 0.07063 \u001b[0m | \u001b[95m 0.8863  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8342  \u001b[0m | \u001b[0m 0.8819  \u001b[0m | \u001b[0m 0.01543 \u001b[0m | \u001b[0m 7.11    \u001b[0m | \u001b[0m 59.96   \u001b[0m | \u001b[0m 0.01434 \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.03283 \u001b[0m | \u001b[0m 0.0692  \u001b[0m | \u001b[0m 0.9731  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.831   \u001b[0m | \u001b[0m 0.9921  \u001b[0m | \u001b[0m 0.01046 \u001b[0m | \u001b[0m 8.749   \u001b[0m | \u001b[0m 38.03   \u001b[0m | \u001b[0m 0.02513 \u001b[0m | \u001b[0m 20.09   \u001b[0m | \u001b[0m 0.04195 \u001b[0m | \u001b[0m 0.06058 \u001b[0m | \u001b[0m 0.9768  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.834   \u001b[0m | \u001b[0m 0.803   \u001b[0m | \u001b[0m 0.01312 \u001b[0m | \u001b[0m 8.421   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.01357 \u001b[0m | \u001b[0m 34.93   \u001b[0m | \u001b[0m 0.03241 \u001b[0m | \u001b[0m 0.06941 \u001b[0m | \u001b[0m 0.9218  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8332  \u001b[0m | \u001b[0m 0.9716  \u001b[0m | \u001b[0m 0.01276 \u001b[0m | \u001b[0m 7.737   \u001b[0m | \u001b[0m 59.99   \u001b[0m | \u001b[0m 0.01728 \u001b[0m | \u001b[0m 35.0    \u001b[0m | \u001b[0m 0.03256 \u001b[0m | \u001b[0m 0.0602  \u001b[0m | \u001b[0m 0.9782  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8461  \u001b[0m | \u001b[0m 0.01981 \u001b[0m | \u001b[0m 7.042   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.01575 \u001b[0m | \u001b[0m 34.83   \u001b[0m | \u001b[0m 0.044   \u001b[0m | \u001b[0m 0.06951 \u001b[0m | \u001b[0m 0.8151  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8324  \u001b[0m | \u001b[0m 0.8658  \u001b[0m | \u001b[0m 0.01151 \u001b[0m | \u001b[0m 8.842   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.02791 \u001b[0m | \u001b[0m 20.01   \u001b[0m | \u001b[0m 0.03535 \u001b[0m | \u001b[0m 0.06798 \u001b[0m | \u001b[0m 0.905   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.9656  \u001b[0m | \u001b[0m 0.01255 \u001b[0m | \u001b[0m 7.86    \u001b[0m | \u001b[0m 38.1    \u001b[0m | \u001b[0m 0.01282 \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.03929 \u001b[0m | \u001b[0m 0.0683  \u001b[0m | \u001b[0m 0.9122  \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.8355  \u001b[0m | \u001b[95m 0.933   \u001b[0m | \u001b[95m 0.01826 \u001b[0m | \u001b[95m 8.856   \u001b[0m | \u001b[95m 38.02   \u001b[0m | \u001b[95m 0.02889 \u001b[0m | \u001b[95m 34.95   \u001b[0m | \u001b[95m 0.03039 \u001b[0m | \u001b[95m 0.06084 \u001b[0m | \u001b[95m 0.9823  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8337  \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 0.01164 \u001b[0m | \u001b[0m 7.071   \u001b[0m | \u001b[0m 38.07   \u001b[0m | \u001b[0m 0.01366 \u001b[0m | \u001b[0m 34.96   \u001b[0m | \u001b[0m 0.04891 \u001b[0m | \u001b[0m 0.06397 \u001b[0m | \u001b[0m 0.807   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8341  \u001b[0m | \u001b[0m 0.955   \u001b[0m | \u001b[0m 0.01779 \u001b[0m | \u001b[0m 7.091   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.01963 \u001b[0m | \u001b[0m 34.97   \u001b[0m | \u001b[0m 0.03311 \u001b[0m | \u001b[0m 0.06952 \u001b[0m | \u001b[0m 0.8798  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "{'colsample_bytree': 0.9329603179995134, 'learning_rate': 0.01825623109630194, 'max_depth': 8.855736097331281, 'min_child_weight': 38.018525051333086, 'min_split_gain': 0.028889434484792968, 'num_leaves': 34, 'reg_alpha': 0.030391642810454738, 'reg_lambda': 0.06083735488706339, 'subsample': 0.9823232634212059}\n"
     ]
    }
   ],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "    test_pred_proba = np.zeros(len(X))\n",
    "    \n",
    "    n_splits = 10\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=34623)\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "        # Feature selection\n",
    "        clf = lgb.LGBMClassifier()\n",
    "        selector = clf.fit(X, Y)\n",
    "        fs = SelectFromModel(selector, prefit=True)\n",
    "        train_df = fs.transform(X)\n",
    "        test_df = fs.transform(test)\n",
    "\n",
    "        train_x, train_y = train_df[train_idx], Y[train_idx]\n",
    "        valid_x, valid_y = train_df[valid_idx], Y[valid_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params, n_estimators=100, nthread=4)\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(Y, test_pred_proba)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # set to ignore warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': (.01, .02),\n",
    "        'num_leaves': (20, 35),\n",
    "        'colsample_bytree': (0.8, 1),\n",
    "        'subsample': (0.8, 1),\n",
    "        'max_depth': (7, 9),\n",
    "        'reg_alpha': (.03, .05),\n",
    "        'reg_lambda': (.06, .08),\n",
    "        'min_split_gain': (.01, .03),\n",
    "        'min_child_weight': (38, 60)\n",
    "    }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "    bo.maximize(init_points=5, n_iter=10)\n",
    "    best_params = bo.max['params']\n",
    "    \n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "\n",
    "    print(best_params)\n",
    "    # output copied and stored as params.json\n",
    "    \n",
    "    import json\n",
    "    json.dump(best_params, open('params2.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.8415  \u001b[0m | \u001b[0m 0.01971 \u001b[0m | \u001b[0m 8.745   \u001b[0m | \u001b[0m 54.06   \u001b[0m | \u001b[0m 0.01049 \u001b[0m | \u001b[0m 26.36   \u001b[0m | \u001b[0m 0.04994 \u001b[0m | \u001b[0m 0.06903 \u001b[0m | \u001b[0m 0.9926  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8328  \u001b[0m | \u001b[0m 0.9919  \u001b[0m | \u001b[0m 0.01151 \u001b[0m | \u001b[0m 8.218   \u001b[0m | \u001b[0m 58.65   \u001b[0m | \u001b[0m 0.01025 \u001b[0m | \u001b[0m 29.8    \u001b[0m | \u001b[0m 0.04028 \u001b[0m | \u001b[0m 0.07583 \u001b[0m | \u001b[0m 0.8913  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.9292  \u001b[0m | \u001b[0m 0.01589 \u001b[0m | \u001b[0m 8.94    \u001b[0m | \u001b[0m 48.14   \u001b[0m | \u001b[0m 0.01227 \u001b[0m | \u001b[0m 25.5    \u001b[0m | \u001b[0m 0.0481  \u001b[0m | \u001b[0m 0.07888 \u001b[0m | \u001b[0m 0.9536  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8354  \u001b[0m | \u001b[0m 0.9435  \u001b[0m | \u001b[0m 0.0184  \u001b[0m | \u001b[0m 7.655   \u001b[0m | \u001b[0m 53.89   \u001b[0m | \u001b[0m 0.02835 \u001b[0m | \u001b[0m 27.56   \u001b[0m | \u001b[0m 0.04113 \u001b[0m | \u001b[0m 0.07838 \u001b[0m | \u001b[0m 0.9748  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.8557  \u001b[0m | \u001b[0m 0.01007 \u001b[0m | \u001b[0m 8.565   \u001b[0m | \u001b[0m 54.05   \u001b[0m | \u001b[0m 0.01148 \u001b[0m | \u001b[0m 28.9    \u001b[0m | \u001b[0m 0.04774 \u001b[0m | \u001b[0m 0.06122 \u001b[0m | \u001b[0m 0.9773  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.833   \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 0.01197 \u001b[0m | \u001b[0m 7.658   \u001b[0m | \u001b[0m 38.09   \u001b[0m | \u001b[0m 0.02434 \u001b[0m | \u001b[0m 20.04   \u001b[0m | \u001b[0m 0.04896 \u001b[0m | \u001b[0m 0.06949 \u001b[0m | \u001b[0m 0.8583  \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.8368  \u001b[0m | \u001b[95m 0.8852  \u001b[0m | \u001b[95m 0.01948 \u001b[0m | \u001b[95m 8.46    \u001b[0m | \u001b[95m 38.06   \u001b[0m | \u001b[95m 0.01329 \u001b[0m | \u001b[95m 35.0    \u001b[0m | \u001b[95m 0.04822 \u001b[0m | \u001b[95m 0.06604 \u001b[0m | \u001b[95m 0.871   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.8373  \u001b[0m | \u001b[95m 0.8148  \u001b[0m | \u001b[95m 0.01946 \u001b[0m | \u001b[95m 8.608   \u001b[0m | \u001b[95m 38.1    \u001b[0m | \u001b[95m 0.019   \u001b[0m | \u001b[95m 34.96   \u001b[0m | \u001b[95m 0.04459 \u001b[0m | \u001b[95m 0.07759 \u001b[0m | \u001b[95m 0.8513  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.8523  \u001b[0m | \u001b[0m 0.01176 \u001b[0m | \u001b[0m 7.084   \u001b[0m | \u001b[0m 38.2    \u001b[0m | \u001b[0m 0.02045 \u001b[0m | \u001b[0m 35.0    \u001b[0m | \u001b[0m 0.03955 \u001b[0m | \u001b[0m 0.06619 \u001b[0m | \u001b[0m 0.9937  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8366  \u001b[0m | \u001b[0m 0.8538  \u001b[0m | \u001b[0m 0.01709 \u001b[0m | \u001b[0m 8.84    \u001b[0m | \u001b[0m 38.01   \u001b[0m | \u001b[0m 0.01825 \u001b[0m | \u001b[0m 34.98   \u001b[0m | \u001b[0m 0.03829 \u001b[0m | \u001b[0m 0.06358 \u001b[0m | \u001b[0m 0.985   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8366  \u001b[0m | \u001b[0m 0.8447  \u001b[0m | \u001b[0m 0.01659 \u001b[0m | \u001b[0m 8.99    \u001b[0m | \u001b[0m 38.18   \u001b[0m | \u001b[0m 0.0236  \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.03442 \u001b[0m | \u001b[0m 0.07956 \u001b[0m | \u001b[0m 0.8672  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8356  \u001b[0m | \u001b[0m 0.854   \u001b[0m | \u001b[0m 0.01334 \u001b[0m | \u001b[0m 8.838   \u001b[0m | \u001b[0m 38.07   \u001b[0m | \u001b[0m 0.02352 \u001b[0m | \u001b[0m 34.93   \u001b[0m | \u001b[0m 0.03452 \u001b[0m | \u001b[0m 0.07667 \u001b[0m | \u001b[0m 0.8103  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.876   \u001b[0m | \u001b[0m 0.01188 \u001b[0m | \u001b[0m 8.81    \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.02509 \u001b[0m | \u001b[0m 34.93   \u001b[0m | \u001b[0m 0.03076 \u001b[0m | \u001b[0m 0.07075 \u001b[0m | \u001b[0m 0.8289  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8367  \u001b[0m | \u001b[0m 0.8866  \u001b[0m | \u001b[0m 0.01805 \u001b[0m | \u001b[0m 7.319   \u001b[0m | \u001b[0m 38.11   \u001b[0m | \u001b[0m 0.02839 \u001b[0m | \u001b[0m 34.94   \u001b[0m | \u001b[0m 0.03927 \u001b[0m | \u001b[0m 0.06244 \u001b[0m | \u001b[0m 0.8778  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8362  \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 0.01515 \u001b[0m | \u001b[0m 7.232   \u001b[0m | \u001b[0m 38.02   \u001b[0m | \u001b[0m 0.01246 \u001b[0m | \u001b[0m 34.97   \u001b[0m | \u001b[0m 0.0316  \u001b[0m | \u001b[0m 0.07438 \u001b[0m | \u001b[0m 0.8347  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "{'colsample_bytree': 0.8147947135880511, 'learning_rate': 0.01946062820121581, 'max_depth': 8.607725521050186, 'min_child_weight': 38.104779422806054, 'min_split_gain': 0.018996824484299953, 'num_leaves': 34, 'reg_alpha': 0.044590215958296146, 'reg_lambda': 0.07759324586561436, 'subsample': 0.8512648687844437}\n"
     ]
    }
   ],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "    test_pred_proba = np.zeros(len(X))\n",
    "    \n",
    "    n_splits = 10\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1367457)\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "        # Feature selection\n",
    "        clf = lgb.LGBMClassifier()\n",
    "        selector = clf.fit(X, Y)\n",
    "        fs = SelectFromModel(selector, prefit=True)\n",
    "        train_df = fs.transform(X)\n",
    "        test_df = fs.transform(test)\n",
    "\n",
    "        train_x, train_y = train_df[train_idx], Y[train_idx]\n",
    "        valid_x, valid_y = train_df[valid_idx], Y[valid_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params, n_estimators=100, nthread=4)\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(Y, test_pred_proba)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # set to ignore warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': (.01, .02),\n",
    "        'num_leaves': (20, 35),\n",
    "        'colsample_bytree': (0.8, 1),\n",
    "        'subsample': (0.8, 1),\n",
    "        'max_depth': (7, 9),\n",
    "        'reg_alpha': (.03, .05),\n",
    "        'reg_lambda': (.06, .08),\n",
    "        'min_split_gain': (.01, .03),\n",
    "        'min_child_weight': (38, 60)\n",
    "    }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "    bo.maximize(init_points=5, n_iter=10)\n",
    "    best_params = bo.max['params']\n",
    "    \n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "\n",
    "    print(best_params)\n",
    "    # output copied and stored as params.json\n",
    "    \n",
    "    import json\n",
    "    json.dump(best_params, open('params3.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.834   \u001b[0m | \u001b[0m 0.8049  \u001b[0m | \u001b[0m 0.01419 \u001b[0m | \u001b[0m 8.289   \u001b[0m | \u001b[0m 57.73   \u001b[0m | \u001b[0m 0.0221  \u001b[0m | \u001b[0m 23.64   \u001b[0m | \u001b[0m 0.03556 \u001b[0m | \u001b[0m 0.06523 \u001b[0m | \u001b[0m 0.8815  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8335  \u001b[0m | \u001b[0m 0.9196  \u001b[0m | \u001b[0m 0.01121 \u001b[0m | \u001b[0m 7.926   \u001b[0m | \u001b[0m 41.79   \u001b[0m | \u001b[0m 0.01644 \u001b[0m | \u001b[0m 30.37   \u001b[0m | \u001b[0m 0.04919 \u001b[0m | \u001b[0m 0.07983 \u001b[0m | \u001b[0m 0.926   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8353  \u001b[0m | \u001b[95m 0.8188  \u001b[0m | \u001b[95m 0.01494 \u001b[0m | \u001b[95m 7.234   \u001b[0m | \u001b[95m 48.02   \u001b[0m | \u001b[95m 0.02922 \u001b[0m | \u001b[95m 30.8    \u001b[0m | \u001b[95m 0.03877 \u001b[0m | \u001b[95m 0.07062 \u001b[0m | \u001b[95m 0.8606  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8341  \u001b[0m | \u001b[0m 0.8876  \u001b[0m | \u001b[0m 0.0158  \u001b[0m | \u001b[0m 8.743   \u001b[0m | \u001b[0m 56.29   \u001b[0m | \u001b[0m 0.01104 \u001b[0m | \u001b[0m 25.55   \u001b[0m | \u001b[0m 0.0494  \u001b[0m | \u001b[0m 0.06515 \u001b[0m | \u001b[0m 0.8318  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8329  \u001b[0m | \u001b[0m 0.9648  \u001b[0m | \u001b[0m 0.01249 \u001b[0m | \u001b[0m 7.564   \u001b[0m | \u001b[0m 39.27   \u001b[0m | \u001b[0m 0.01019 \u001b[0m | \u001b[0m 24.17   \u001b[0m | \u001b[0m 0.04755 \u001b[0m | \u001b[0m 0.068   \u001b[0m | \u001b[0m 0.9083  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8341  \u001b[0m | \u001b[0m 0.9404  \u001b[0m | \u001b[0m 0.01836 \u001b[0m | \u001b[0m 7.001   \u001b[0m | \u001b[0m 59.96   \u001b[0m | \u001b[0m 0.01274 \u001b[0m | \u001b[0m 34.94   \u001b[0m | \u001b[0m 0.03202 \u001b[0m | \u001b[0m 0.06054 \u001b[0m | \u001b[0m 0.9724  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8333  \u001b[0m | \u001b[0m 0.9634  \u001b[0m | \u001b[0m 0.01435 \u001b[0m | \u001b[0m 7.016   \u001b[0m | \u001b[0m 59.96   \u001b[0m | \u001b[0m 0.01099 \u001b[0m | \u001b[0m 34.91   \u001b[0m | \u001b[0m 0.03331 \u001b[0m | \u001b[0m 0.07334 \u001b[0m | \u001b[0m 0.9008  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8335  \u001b[0m | \u001b[0m 0.9842  \u001b[0m | \u001b[0m 0.0161  \u001b[0m | \u001b[0m 7.071   \u001b[0m | \u001b[0m 38.03   \u001b[0m | \u001b[0m 0.01196 \u001b[0m | \u001b[0m 20.06   \u001b[0m | \u001b[0m 0.03541 \u001b[0m | \u001b[0m 0.07292 \u001b[0m | \u001b[0m 0.9041  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8346  \u001b[0m | \u001b[0m 0.847   \u001b[0m | \u001b[0m 0.01937 \u001b[0m | \u001b[0m 7.985   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.01402 \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.03523 \u001b[0m | \u001b[0m 0.0755  \u001b[0m | \u001b[0m 0.9619  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8314  \u001b[0m | \u001b[0m 0.9189  \u001b[0m | \u001b[0m 0.01032 \u001b[0m | \u001b[0m 8.892   \u001b[0m | \u001b[0m 59.98   \u001b[0m | \u001b[0m 0.02755 \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 0.03344 \u001b[0m | \u001b[0m 0.06281 \u001b[0m | \u001b[0m 0.982   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8341  \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.01132 \u001b[0m | \u001b[0m 7.574   \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.02329 \u001b[0m | \u001b[0m 34.98   \u001b[0m | \u001b[0m 0.04333 \u001b[0m | \u001b[0m 0.07956 \u001b[0m | \u001b[0m 0.8281  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8346  \u001b[0m | \u001b[0m 0.8106  \u001b[0m | \u001b[0m 0.01108 \u001b[0m | \u001b[0m 8.18    \u001b[0m | \u001b[0m 38.0    \u001b[0m | \u001b[0m 0.02677 \u001b[0m | \u001b[0m 34.82   \u001b[0m | \u001b[0m 0.03956 \u001b[0m | \u001b[0m 0.06722 \u001b[0m | \u001b[0m 0.8168  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8341  \u001b[0m | \u001b[0m 0.9008  \u001b[0m | \u001b[0m 0.0109  \u001b[0m | \u001b[0m 8.929   \u001b[0m | \u001b[0m 38.08   \u001b[0m | \u001b[0m 0.01834 \u001b[0m | \u001b[0m 34.99   \u001b[0m | \u001b[0m 0.03847 \u001b[0m | \u001b[0m 0.07504 \u001b[0m | \u001b[0m 0.8304  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8341  \u001b[0m | \u001b[0m 0.9908  \u001b[0m | \u001b[0m 0.01231 \u001b[0m | \u001b[0m 7.198   \u001b[0m | \u001b[0m 38.14   \u001b[0m | \u001b[0m 0.01265 \u001b[0m | \u001b[0m 35.0    \u001b[0m | \u001b[0m 0.04067 \u001b[0m | \u001b[0m 0.07296 \u001b[0m | \u001b[0m 0.9094  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8348  \u001b[0m | \u001b[0m 0.8759  \u001b[0m | \u001b[0m 0.01302 \u001b[0m | \u001b[0m 7.393   \u001b[0m | \u001b[0m 38.01   \u001b[0m | \u001b[0m 0.02054 \u001b[0m | \u001b[0m 35.0    \u001b[0m | \u001b[0m 0.04762 \u001b[0m | \u001b[0m 0.0615  \u001b[0m | \u001b[0m 0.8085  \u001b[0m |\n",
      "=====================================================================================================================================\n",
      "{'colsample_bytree': 0.8187898779901973, 'learning_rate': 0.014935251634429077, 'max_depth': 7.234206538343034, 'min_child_weight': 48.01907594802781, 'min_split_gain': 0.02921818707966474, 'num_leaves': 30, 'reg_alpha': 0.03876728730941738, 'reg_lambda': 0.0706163724907318, 'subsample': 0.8606457408905}\n"
     ]
    }
   ],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "    test_pred_proba = np.zeros(len(X))\n",
    "    \n",
    "    n_splits = 10\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=12321)\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "        # Feature selection\n",
    "        clf = lgb.LGBMClassifier()\n",
    "        selector = clf.fit(X, Y)\n",
    "        fs = SelectFromModel(selector, prefit=True)\n",
    "        train_df = fs.transform(X)\n",
    "        test_df = fs.transform(test)\n",
    "\n",
    "        train_x, train_y = train_df[train_idx], Y[train_idx]\n",
    "        valid_x, valid_y = train_df[valid_idx], Y[valid_idx]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params, n_estimators=100, nthread=4)\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=False, early_stopping_rounds=100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(Y, test_pred_proba)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # set to ignore warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': (.01, .02),\n",
    "        'num_leaves': (20, 35),\n",
    "        'colsample_bytree': (0.8, 1),\n",
    "        'subsample': (0.8, 1),\n",
    "        'max_depth': (7, 9),\n",
    "        'reg_alpha': (.03, .05),\n",
    "        'reg_lambda': (.06, .08),\n",
    "        'min_split_gain': (.01, .03),\n",
    "        'min_child_weight': (38, 60)\n",
    "    }\n",
    "\n",
    "    bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "    bo.maximize(init_points=5, n_iter=10)\n",
    "    best_params = bo.max['params']\n",
    "    \n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "\n",
    "    print(best_params)\n",
    "    # output copied and stored as params.json\n",
    "    \n",
    "    import json\n",
    "    json.dump(best_params, open('params4.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
